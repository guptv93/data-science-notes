{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\real}[1]{\\mathbb{#1}}\n",
    "\\newcommand{\\expect}{\\mathrm{E}}\n",
    "\\newcommand{\\prob}{\\mathrm{P}}\n",
    "\\newcommand{\\v}{\\mathrm{var}}\n",
    "\\newcommand{\\Comb}[2]{{}^{#1}C_{#2}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs; Expected Values and Variance; CDFs \n",
    "*Lecture 08*  \n",
    "\n",
    "\n",
    "We study continuous random variables and their probability density functions. PDFs replace PMFs from the discrete case. We then study expected values and variances of continuous random variables. We also look at Cumulative Density Functions and then study common types of CRVs, namely Uniform, Exponential and Normal Distributions.\n",
    "\n",
    "Probability Density Function (PDF) of a continuous random variable represents the probability density (probability per unit length) near each value of the random variable. PDF of a Random Variable $X$ is denoted by $f_X(x)$.  \n",
    "$$\\prob(a \\leq X \\leq a + \\delta) = f_X(x)\\cdot\\delta$$  \n",
    "\n",
    "Thus $f_X(a)$ gives the probability of the $X$ lying in a very very small interval close to $a$ per unit length. It is important to understand that even though PDF is used to calculate event probabilities, $f_X(a)$ is not the probability of any particular event $a$. Also $f_X(x)$ is not less than or equal to one for all $ x \\in \\real{R}$. Rather, $$ \\int_{-\\infty}^{+\\infty}f_X(x)dx = 1 $$   \n",
    "\n",
    "Notice that by taking $\\delta = 0$, we get $$\\prob(X=x) = 0$$  \n",
    "This tells us that at any single point, the PDF is exactly zero. Why? If $X$ denotes height of a person, then can't we say that the height of a person is 180.5 cms with certainty? The thing to notice here is that even in the previous statement, we are specifying a range. The range covers all the double, triple, ... precision floating points greater than 180.5 and less than 180.6  \n",
    "\n",
    "**Expected Value and Variance**\n",
    "$$\\expect[X] = \\int_{-\\infty}^{+\\infty}xf_X(x)dx $$  \n",
    "\n",
    "The interpretation of expected values is still the same. It is the average of the random variable in large number of independent repetitions of the experiment.\n",
    "\n",
    "Properties:\n",
    "* If $X \\geq 0$, then $\\expect[X] \\gt 0$\n",
    "* If $a < X < b$, then $a < \\expect[X] < b$\n",
    "* Expectation of a function : $\\expect[g(X)] = \\int_{-\\infty}^{+\\infty}g(x)f_X(x)dx $\n",
    "* Linearity of Expectation : $\\expect[aX + b] = a\\expect[X] + b$\n",
    "\n",
    "\n",
    "Variance is exactly similar to its discrete counterpart.\n",
    "\\begin{align}\n",
    "\\v(X) &= \\expect[(X - \\mu)^2]\\\\\n",
    "\\v(aX+b) &= a^2\\v(X) \\\\ \n",
    "\\v(X) &= \\expect[X^2] - (\\expect[X])^2 \\\\ \n",
    "\\end{align}  \n",
    "\n",
    "Below we see the Uniform and Exponential Continuous RVs. The derivations don't introduce any new techniques and can be safely skipped over.  \n",
    "\n",
    "If $X$ is uniform between $a$ and $b$, then\n",
    "$$\\expect[X] = \\frac{1}{a-b}$$\n",
    "$$\\v[X] = \\expect[X^2] - (\\expect[X])^2 = \\frac{(b-a)^2}{12}$$  \n",
    "\n",
    "If $X$ is an exponential random variable with parameter $\\lambda > 0$,\n",
    "$$\n",
    "f_X(x) = \n",
    "\\begin{cases}\n",
    "\\lambda e^{-\\lambda x} &x\\geq0\\\\\n",
    "0 &x<0\\\\\n",
    "\\end{cases}\n",
    "$$  \n",
    "<img src=\"images/exp_rv.png\">  \n",
    "\n",
    "$$\\expect[X] = \\frac{1}{\\lambda}$$\n",
    "$$\\expect[X^2] = 2 / \\lambda^2$$\n",
    "$$\\v(X) = \\frac{1}{\\lambda^2}$$  \n",
    "\n",
    "**Cumulative Distribution Function (CDF)**  \n",
    "$$F_X(x) = \\prob(X\\leq x)$$\n",
    "\n",
    "This is a concept that can be used to describe both, probability laws for discrete as well as continuous random variables. The value of CDF at a point $x$ is equal to the probability of $X$ being less than or equal to $x$.\n",
    "CDF is a non-decreasing function; and as x tends to $+\\infty$ CDF tends to 1.  \n",
    "\n",
    "### Normal Distributions  \n",
    "The General Normal Distribution has two parameters $\\mu$ and $\\sigma^2$, and its PDF is given by \n",
    "$$f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$  \n",
    "\n",
    "X is a Normal Variable with parameters $\\mu$ and $\\sigma^2$ is denoted by $X = N(\\mu,\\sigma^2)$. It is evident from the PDF equation that the distribution is symmetric around $\\mu$. Thus, $\\expect[X] = \\mu$. Using integration by parts it can be proved that $\\v(X) = \\sigma^2$ ($\\sigma$ is the standard deviation of the distribution). We don't prove it mathematically, but the formula seems intuitively coherent. We can see from the Normal PDF formula that if $\\sigma$ increases, the graph becomes wider. This is coherent with standard deviation definition, which says that if s.d. increases, values move away from the mean.  \n",
    "\n",
    "$X = N(0,1)$ is called the Standard Normal Distribution. The simplified equation for Standard Normal is given below:\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}$$  \n",
    "\n",
    "Finally, let us talk about a linear function of Normal Random Variable. Let $X = N(\\mu,\\sigma^2)$. Then, $Y = aX + b$. We know that,\n",
    "\\begin{align}\n",
    "\\expect[Y] = a\\mu + b \\\\\n",
    "\\v[Y] = a^2\\sigma^2\n",
    "\\end{align}  \n",
    "\n",
    "We will *prove later* that $Y$ is also a Normal Variable. \n",
    "$$Y = N(a\\mu + b, a^2\\sigma^2)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning on an Event; Joint PDFs of Multiple RVs\n",
    "*Lecture 09*  \n",
    "\n",
    "\n",
    "Conditional PDF of $X$ given $X \\in A$ is defined as follows:\n",
    "$$\n",
    "f_{X|X\\in A}(x) = \n",
    "\\begin{cases}\n",
    "0 & x \\not\\in A \\\\\n",
    "\\frac{f_X(x)}{\\prob(A)} & x \\in A \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "Basic Rules of Conditional PDFs given an event (and their counterparts in Discrete Form):  \n",
    "<img src=\"images/cond_pdf\">  \n",
    "\n",
    "Conditional Expected Value:  \n",
    "$$ \\expect[X|A] = \\int xf_{X|A}(x)dx $$\n",
    "$$ \\expect[g(X)|A] = \\int g(x)f_{X|A}(x)dx $$\n",
    "\n",
    "Finally, the total probability and expectation theorems are as follows:  \n",
    "$$ \n",
    "f_{X}(x) = \\prob(A_1)f_{X|A_1}(x) + \\dots + \\prob(A_n)f_{X|A_n}(x) \\\\\n",
    "\\expect[X] = \\prob(A_1)\\expect[X|A_1] + \\dots + \\prob(A_n)\\expect[X|A_n] \\\\\n",
    "$$  \n",
    "\n",
    "The total probability/expectation theorems are useful if the PDF of a RV is defined piecewise (different function in different scenarios). For example, the PDF might be piecewise uniform or uniform on one piece and geometric on another. This is also useful in case of Mixed Random Variables (Discrete in some scenarios, Continuous in others). In that case we can work with the CDF, instead of PDF. Remember that CDF is defined for both Discrete and Random cases. \n",
    "\n",
    "\n",
    "### Memorylessness of Exponential PDFs\n",
    "\n",
    "Consider the lifetime of a light bulb. Suppose the bulb is switched on at time 0 and it stops functioning at an instant given by random variable $T$.\n",
    "$$P(T > x) = e^{-\\lambda x}, \\textrm{ for } x \\geq 0 $$  \n",
    "\n",
    "If you have to buy such an exponential light bulb, will you prefer a more expensive new light bulb or a cheaper \"used\" light bulb?   \n",
    "\n",
    "Suppose the time right now is $t$. $X$ denotes the time for which the bulb will be functional, with respect to instant $t$. Now given that we know that the bulb has been functioning from time 0 to time $t$, what is the probability of it functioning for more than $x$ time (probability of $X > x$)?  \n",
    "\\begin{align}\n",
    "\\prob(X \\geq x \\mid T > t) &= \\frac{\\prob(T-t > x \\; , \\; T > t)}{\\prob(T>t)} \\\\\n",
    "&= \\frac{\\prob(T > t+x \\; , \\; T > t)}{\\prob(T>t)} \\\\\n",
    "&= \\frac{\\prob(T > t+x)}{\\prob(T>t)} \\\\\n",
    "&= \\frac{e^{-\\lambda (t+x)}}{e^{-\\lambda t}} \\\\\n",
    "&= e^{-\\lambda x} \\\\\n",
    "&= \\prob(T>x) \\\\\n",
    "\\end{align}  \n",
    "\n",
    "Thus, the probability of a used light bulb working for $x$ time is the same as the probability of a new light bulb working for $x$ time. Go for the cheaper used one!!  \n",
    "  \n",
    "### Joint PDFs of Multiple RVs\n",
    "\n",
    "$f_{X,Y}(a,b)$ represents the probability that the random variables $X$ and $Y$ associated with the same experiment take values very very close to a and b.  \n",
    "$$\\prob(a < X < a+\\epsilon , b < Y < b+\\delta) = f_{X,Y}(a,b) \\cdot \\epsilon \\cdot \\delta$$\n",
    "We can view $f_{X,Y}(a,b)$ as the probability per unit area in the vicinity of $(a,b)$.  \n",
    "\n",
    "We say that two continuous random variables associated with the same experiment are **jointly continuous** and can be described in terms of a **joint PDF** $f_{X,Y}$ if $f_{X,Y}$ is a non-negative function that satisfies \n",
    "$$ \\prob((X,Y) \\in B) = \\iint_{(x,y)\\in B}f_{X,Y}(x,y)dx dy $$\n",
    "for every subset $B$ of the two dimensional plane.   \n",
    "\n",
    "The Marginal PDF of $f_X$ of $X$ is given by  \n",
    "$$ f_X(x) = \\int_{-\\infty}^{+\\infty}f_{X,Y}(x,y)dy $$  \n",
    "\n",
    "Basically marginal distribution of $X$ tells us how many events (per unit length) take values very close to $x$. For that we need to add all the events that had values of $X$ close to $x$, and the value of $Y$ for these events can be whatever we want (from $-\\infty$ to $+\\infty$). Thus, we calculate the volume of a thick slice of the Joint PDF graph, parallel to Y-axis and with width $\\delta$ (from $a$ to $a+\\delta$) on X-axis. This volume will be equal to $f_X(a)\\cdot\\delta$. See Ex 3.10 or Lec 9.8 to understand marginal PDF with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning on another Random Variable (X on Y); Independence; Bayesâ€™ Rule \n",
    "*Lecture 10*\n",
    "\n",
    "**Conditioning variable X over variable Y**  \n",
    "We use Joint PDFs, explained in the previous lecture, to define conditionality between multiple random variables.\n",
    "\\begin{align}\n",
    "&\\prob(x < X < x+\\delta \\mid y < Y < y + \\epsilon) = \\frac{f_{X,Y}(x,y)\\cdot\\delta\\cdot\\epsilon}{f_Y(y)\\cdot\\epsilon} = f_{X|Y}(x|y)\\cdot\\delta  \\\\\n",
    "\\implies & f_{X\\mid Y}(x\\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n",
    "\\end{align}  \n",
    "\n",
    "**Total Probability and Expectation Theorems**\n",
    "$$f_X(x) = \\int_{-\\infty}^{+\\infty} f_Y(y)f_{X|Y}(x|y)dy$$\n",
    "The above theorem comes directly from the equation of marginal PDF using joint PDF.  \n",
    "\n",
    "$$\\expect[X] = \\int_{-\\infty}^{+\\infty}f_Y(y)\\expect[X| Y=y]dy$$\n",
    "\n",
    "**Independence**  \n",
    "$X$ and $Y$ are independent iff:\n",
    "$$f_{X|Y}(x|y) = f_{X}(x)$$\n",
    "Equivalently,\n",
    "$$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$  \n",
    "\n",
    "If $X$ and $Y$ are independent:\n",
    "* $\\expect[XY] = \\expect[X]\\expect[Y]$  \n",
    "* $\\v(X + Y) = \\v(X) + \\v(Y)$  \n",
    "\n",
    "### Stick Problem\n",
    "Break a stick of length $l$ twice, once at $X$ and then at $Y$. $X$ is uniform in $[0,l]$ and $Y$ is uniform in $[0,X]$. \n",
    "$$\n",
    "f_{X,Y}(x,y) = f_X(x) f_{Y|X}(y|x) = \\frac{1}{lx}\\\\ \n",
    "\\textrm{where  }0\\leq y \\leq x \\leq l\n",
    "$$\n",
    "\n",
    "Here, \n",
    "$$ f_Y(y) = \\int_y^l \\frac{1}{lx}dx = \\frac{1}{l}log(\\frac{l}{y})$$  \n",
    "\n",
    "Calculating Expected Value for $Y$ directly is gonna be pretty difficult. So we use the Total Expectation Theorem:  \n",
    "\\begin{align}\n",
    "\\expect[Y] &= \\int_0^l\\frac{1}{l} \\expect[Y|X=x]dx \\\\\n",
    "&= \\int_0^l\\frac{1}{l} \\frac{x}{2}dx \\\\\n",
    "&= \\frac{1}{2}\\expect[X] \\\\\n",
    "&= \\frac{1}{2}\\cdot\\frac{l}{2} \\\\\n",
    "&= \\frac{l}{4} \\\\\n",
    "\\end{align}\n",
    "\n",
    "### Independent Normals\n",
    "Here $X$ and $Y$ are independent normal variables and we are interested in studying their Joint PDF given by\n",
    "$$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$  \n",
    "\n",
    "See [Lec 10.7](https://youtu.be/qOQxeYGOIag) for more details.\n",
    "\n",
    "### Bayesian Inference\n",
    "Bayesian Inference is all about infering the value of an unobserved RV $X$ on the basis of an observed RV $Y$. Instead of depending on the prior probability distribution of X, given by $p_X(.)$, we depend on the posterior probability distribution $p_{X|Y}(.)$ We have seen the Bayes' Rule for general probability and events. Here we translate it into PMF notation.  \n",
    "\n",
    "<img src=\"images/bayes_pdf.png\">\n",
    "\n",
    "It is not necessary that the observed RV and unobserved RV are both Discrete or Continuous. One might be continuous and other might be discrete. Bayes' Rule for these situations can be derived from the definition of Joint Probabilities using similar techniques. See Lec 10.9 for more details.  \n",
    "\n",
    "Finally we are shown two scenarios where this Mixed Bayes' Rule is used. First is the Binary Signal Detection. Here we observe a continuous signal over the wire, and we need to infer whether it is 0 or 1 (discrete). Second is Coin Bias Inference. Here the observed RV is the number of heads (discrete). The variable to be infered (bias) is continuous between 0 and 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\C}{\\mathcal{C}}\n",
    "$\n",
    "# Essence of Linear Algebra - I\n",
    "\n",
    "*This is meant to accompany each corresponding video in the Essence of Linear Algebra Series*  \n",
    "\n",
    "\n",
    "### What are Vectors?\n",
    "\n",
    "Is it a list of numbers (CS student perspective)? Is it an arrow in space with a length and direction and that can be moved around in space (physics student perspective)? We define it with a Maths perspective. For a maths student, it is anything for which the two operations, vector addition and scalar multiplication, make sense.\n",
    "\n",
    "The video also goes over vector addition and multiplication.  \n",
    "\n",
    "\n",
    "### Linear Combinations, Span, and Basis Vectors\n",
    "\n",
    "Linear Combination : Scaling and adding two vectors.\n",
    "\n",
    "Span : All vectors that can be obtained by taking linear combinations of a set of vectors is the span of the particular set of vectors.\n",
    "\n",
    "Vector Space : A set of vectors in which addition and scalar multiplication are closed operations. All spans constitute a vector space.\n",
    "\n",
    "Linearly Independent : Suppose we have a set of vectors. If adding another vector doesn’t add anything to the span of the original vectors, then the added vector is a linear combination of the original vectors. In this case, the set of vectors (original plus last added) are Linearly Dependent. On the other hand, if the newly added vector adds more vectors to the span of the previous set, then the new vector is said to add a Dimension to the span.\n",
    "\n",
    "Basis Vectors : The basis of a vector space is a set of linearly independent vectors that span the full space.\n",
    "\n",
    "Unit Vector : A vector with length 1. In this text, we also use the term 'unit vectors' to denote $\\hat{i}, \\hat{j}$ and their transformations $\\hat{i'}, \\hat{j'}$.\n",
    "\n",
    "Dimension : In the definition of Linear Independence, if the newly added vector adds more vectors to the span, then the new vector is said to add a dimension to the span. The number of vectors in the basis of a vector space is the dimension of the vector space. Now we know that each vector space can have infinitely many basis sets. Do all of these basis sets have the same number of vectors? Yes! [Proof: Any subspace basis has same number of elements (video) |Khan Academy](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/null-column-space/v/proof-any-subspace-basis-has-same-number-of-elements)  \n",
    "\n",
    "\n",
    "### Linear Transformations and Matrices\n",
    "\n",
    "A linear transformation of a space is when you move all the grid lines in a way that they are still parallel and equidistant from each other. The points of intersections of these grid lines move from one location to the other. This movement is referred to as Linear Transformation of the space. The original location of (1,1) is where the first line parallel to Y axis (name line A) meets the first line parallel to X axis (line B). The new location of (1,1) is where the linear transformation of line A meets the linear transformation of line B. It is obvious that if the vector representing the point in original space was $1 \\hat{i} + 1 \\hat{j}$, then the vector depicting the new point will be $1\\hat{i'} + 1\\hat{j'}$, where $\\hat{i'}, \\hat{j'}$ are transformed $\\hat{i},\\hat{j}$ respectively.\n",
    "\n",
    "A matrix is a horizontal list of vectors. Matrix helps us concisely denote a linear transformation of space. It lists the unit vectors of the transformed space. Multiplying a vector with a matrix gives the transformed vector.\n",
    "\n",
    "Multiplying a vector with a matrix can be thought of denoting a linear combination of the column vectors of the matrix. Each column vector is first scaled by the corresponding row entry in the vector and then all these scaled column vectors are added. Thus the original vector (before the multiplication) denotes a point in the new co-ordinate space (where the column vectors of the matrix are the unit vectors). The vector obtained from multiplication denotes the re-positioned point with coordinates in original space.  \n",
    "\n",
    "\n",
    "### Matrix Multiplication as Composition\n",
    "\n",
    "Many a times we want to sequentially apply multiple transformations to the grid world, one after the other. This is called a Composition of Linear Transformations. It is easy to understand why a composition of linear transformations will be a linear transformation all in itself (the gridlines will still be parallel and the origin would not have moved). Thus the Composite Linear Transformation can be expressed in the matrix form.  \n",
    "\n",
    "Suppose we have an initial co-ordinate system $\\mathcal{C}$ with unit vectors $<\\hat{i},\\hat{j}>$. We represent this as $\\mathcal{C} = <\\hat{i},\\hat{j}>$. We learnt that when we apply transformation $M = [\\vec{m_1},\\vec{m_2}]$ to $\\mathcal{C}$, then $\\hat{i},\\hat{j}$ land on $\\vec{m_1},\\vec{m_2}$ respectively and we get the new co-ordinate system $\\mathcal{C}' = <\\hat{i'},\\hat{j'}> = <\\vec{m_1},\\vec{m_2}>$. When we apply two tranformations, first $M = [\\vec{m_1},\\vec{m_2}]$ and second $N = [\\vec{n_1},\\vec{n_2}]$, unit vectors of which co-ordinate system land on $\\vec{n_1},\\vec{n_2}$? Basically, what does applying linear transformations sequentially mean? Which grid $(\\mathcal{C}$ or $\\mathcal{C'})$ is the second transformation applied to? All this confusion is worsened by the example transformations used in the video (the old and new grids overlap after the rotation applied in the video). The understanding to this is intuitive but the argument below will help build the intuition.\n",
    "\n",
    "Every transformation is applied to the entire space (all the points in it). The different co-ordinate systems $C,C'$ (and their unit vectors) are just different ways of denoting the same points. If I represent a vector $\\vec{x}$ in $C$, then $M\\vec{x}$ gives me the co-ordinates of the transformed $\\vec{x}$ in $C$. Same would be true if $\\vec{x}$ was denoted in $C'$. Irrespective of the co-ordinate system, $\\vec{x}$ will always land on the same spot. Just the co-ordinates of the spot would be different.\n",
    "\n",
    "\"When we apply two tranformations, first $M = [\\vec{m_1},\\vec{m_2}]$ and second $N = [\\vec{n_1},\\vec{n_2}]$, unit vectors of which co-ordinate system land on $\\vec{n_1},\\vec{n_2}$?\"  \n",
    "To answer this, first understand that the columns of a matrix $M$ can represent unit vectors in any co-ordinate system. If I represent points and vectors in co-ordinate system $\\mathcal{A} = <\\hat{a_x},\\hat{a_y}>$, then the columns represent linear transformations of $\\hat{a_x}, \\hat{a_y}$ in co-ordinate system $\\mathcal{A}$. Same goes for $\\mathcal{C}$ and $\\mathcal{C'}$.  \n",
    "Building on the understanding above, we can now analyze how composite linear transformation works. We first apply transformation $M = [\\vec{m_1},\\vec{m_2}]$ and then $N = [\\vec{n_1},\\vec{n_2}]$. $\\vec{m_1},\\vec{m_2}$ give the location of linear transformations of $\\hat{i}, \\hat{j}$. This location is in $C$, so when the column vectors of $M$ are multiplied by $N$, the unit-vectors of $C$ are moved to $\\vec{n_1},\\vec{n_2}$, and the final output is the linear transformation (caused by $N$) of the linear transformation (caused by $M$) of $\\hat{i},\\hat{j}$.  \n",
    "\n",
    "\n",
    "### Three Dimensional Linear Transformations\n",
    "\n",
    "Here we discuss the case where instead of 2-D flat plane we have 3-D space. We discuss linear transformations in 3-D.\n",
    "\n",
    "\n",
    "### The Determinant\n",
    "\n",
    "A Determinant represents the number by which the unit square/cube is squished or stretched after a 2-D/3-D transformation. The value of the determinant is (-ve) if the orientation of the grid has been flipped by the transformation. Negative value of the determinant can be visualized as the area of unit square slowly reducing to zero and then increasing on the other side of orientation of X-axis and Y-axis.  \n",
    "\n",
    "\n",
    "### Inverse Matrices, Column Space and Null Space\n",
    "\n",
    "Matrices are mostly used to solve systems of linear equations. The process to do this is Gaussian Elimination and Row Echelon Form which we discuss in the coming note. Here we discuss the intuition behind this method of solving linear equations. In this section we consider only square matrices (number of equations is equal to the number of unknowns). \n",
    "\n",
    "Here we consider only square matrices. The case of squishing the vector space is represented by determinant of matrix being 0.  If a matrix multiplication doesn’t squish the space, then another matrix transformation can bring each vector to exactly where it was before the transformation. This new matrix is known as the inverse of previous matrix. \n",
    "\n",
    "The case where the transformation squishes the space (the determinant is zero) is more complicated. No function can un-squish the space.  We may still get a solution but for that we need to be lucky. The transformed vector needs to lie in the squished space.\n",
    "\n",
    "Rank is defined as the dimension of the column span. It is obvious that it is the number of independent columns in the transformation matrix. Our next note describes what pivot columns are and how the rank is equal to the number of pivot columns.\n",
    "\n",
    "The video also helps get an intuitive understanding of why rank(column space) + rank(null space) = n.  \n",
    "\n",
    "\n",
    "### Nonsquare matrices as transformations between dimensions\n",
    "\n",
    "A 2x2 matrix transforms from a 2-D co-ordinate system to a 2-D co-ordinate system. Similarly for a 3x3 matrix. What about non-square matrices? Non-square matrices are transformations between dimensions. A 2x3 matrix is a transformation that takes in a vector in 3-D and gives out a vector in 2-D. Thus it reduces the dimension by 1. Observe that this is not the same as a 3x3 matrix that reduces the space to a flat plane. The co-ordinates of the output vector are still in 3-D, though the basis of the column space has only two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

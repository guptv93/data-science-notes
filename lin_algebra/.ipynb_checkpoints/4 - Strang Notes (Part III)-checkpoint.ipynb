{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\mmat}[1]{\\begin{bmatrix} #1 \\end{bmatrix}}\n",
    "\\newcommand{\\C}{\\mathcal{C}}\n",
    "$\n",
    "\n",
    "# Vector Decomposition\n",
    "*Skip over the lectures that have not been mentioned in the notes.*\n",
    "\n",
    "### Lecture 21 (Eigenvalues and Eigenvectors)\n",
    "\n",
    "You can think of a matrix as a linear mapping, using $f(x) = Ax$. For a matrix, we can find vector(s) $x$ such\n",
    "that $f(x)$ is parallel to $x$. Formally:\n",
    "$$Ax = λx$$\n",
    "where λ is a scalar, called the eigenvalue. The $x$s are called the eigenvectors.\n",
    "\n",
    "**Example 1: Projection Matrix**  \n",
    "Say there is a projection matrix, which projects stuff onto a plane: when a vector is already in the plane,\n",
    "it won’t change. So they will be eigenvector with eigenvalue 1. We can say there are two perpendicular\n",
    "eigenvectors that span the plane. Are there any other eigenvectors? (Intuitively, we expect to find one, since\n",
    "we are in a 3D space) Yes, find the normal vector that goes through the origin. This will become a 0 vector on projecting - so this one is an eigenvector with eigenvalue 0.\n",
    "\n",
    "**Example 2: Permutation Matrix**  \n",
    "Let $$A = \\mmat{0&1\\\\1&0}$$\n",
    "The eigenvectors would be vectors that remain same or get scaled up or down when their 1st and 2nd elements are exchanged. Thus $\\mmat{1\\\\1}$ will be eigenvector with eigenvalue 1 and $\\mmat{1\\\\-1}$ with eigenvalue -1.  \n",
    "\n",
    "**Properties (accepted as is, without proof)**  \n",
    "1. n × n matrices will have n eigenvalues, but they might be repeated. (How is this possible for rotation matrix? Complex eigenvalues.)\n",
    "2. Sum of the eigenvalues equals the trace of the matrix (sum of the diagonal entries).\n",
    "3. The product of the eigenvalues is equal to the value of the determinant.\n",
    "\n",
    "**Calculating Eigenvectors**  \n",
    "We know that $(A - λI)x=\\vec{0}, s.t. x \\neq \\vec{0}$. Thus, $(A - λI)$ is a singular matrix that squishes space. Therefore, $det(A - λI) = 0$.  \n",
    "How do we find eigenvectors? If we know the eigenvalues, then solving $(A - λI)x=\\vec{0}$ is just Gaussian Elimination. How do we find eigenvalues? By solving $det(A - λI) = 0$. Also if you try solving this \"determinant is zero\" equation for a 2x2 matrix, you get a quadratic equation of the form $\\lambda^2 -(a+d)\\lambda + (ad - bc) = 0$. Solving this equation gives one an understanding of why the trace $(a+d)$ and the determinant $(ad-bc)$ are sum and product of the eigenvalues (atleast for 2x2 matrix).  \n",
    "\n",
    "### Lecture 22 (Diagonalization and Powers of A)\n",
    "\n",
    "This is a revision of what we saw in the Essence of Linear Algebra series, but presented in a more calculation-oriented way.\n",
    "\n",
    "For the first part, read the section titled \"7.3 Diagonalization of A Matrix\" from [this link](http://theyearlyprophet.com/linalg.pdf). Basically, this tells us that if we have a transformation matrix $A$ of dimension $n \\times n$, and it has **n independent eigenvectors**, then we can construct a $n\\times n$ matrix $S$ such that each column of $S$ is an eigenvector. Also, let $\\Lambda$ be the diagonal matrix, whose diagonal elements are the eigenvalues of the corresponding columns of $S$. In such a case,  \n",
    "$\\qquad AS = S\\Lambda$  \n",
    "$\\implies A = S\\Lambda S^{-1}$  *(S is non-singular from our assumption that all its columns are independent)*  \n",
    "**(this is the diagonalized form of A)**  \n",
    "$\\implies A^2 = (S\\Lambda S^{-1})(S\\Lambda S^{-1}) = S\\Lambda^2S^{-1}$  \n",
    "$\\implies A^2S = S\\Lambda^2$  \n",
    "$\\implies A^kS = S\\Lambda^k$   \n",
    "This shows that all powers of A have the same eigenvectors, and the eigenvalues are powers of the original eigenvalues. Thus, eigenvectors (and diagonalization) gives us a very easy way of representing and calculating powers of a matrix. The usual linear transformation of basis vectors doesn't provide a very useful picture.  \n",
    "\n",
    "Now comes the second part. $u_0$ is a random given vector. \n",
    "$$u_{k+1} = Au_k$$\n",
    "Thus, $u_k = A^ku_0$. But how do we actually calculate $u_k$? To really solve it, we express the initial vector as a linear combination of the $n$ linearly independent eigenvectors of $A$.  \n",
    "$u_0 = c_1x_1 + c_2x_2 + \\dots + c_nx_n$  \n",
    "$\\implies u_k = c_1 \\lambda_1^k x_1 + c_2 \\lambda_2^k x_2 + \\dots + c_n \\lambda_n^k x_n$ *(multiplying both sides by $A$ k times)*  \n",
    "\n",
    "In the [third part of the video](https://youtu.be/13r9QY6cmjc?t=2057), we see how Fibonacci numbers can be solved using the concept of eigenvectors. This is a good to know, but can be skipped over if needed.  \n",
    "\n",
    "\n",
    "### Lecture 25 (Symmetric Matrices; Positive Definiteness)\n",
    "\n",
    "Both symmetric and symmetric positive definite matrices are very important classes of matrices for Singular Value Decomposition.  \n",
    "\n",
    "If $A$ is a $n\\times n$ symmetric matrix, then the following properties hold true:\n",
    "1. All eigenvalues are real. (For proof refer to section 7.6.1 of theyearlyprophet notes)\n",
    "2. We can choose eigenvectors such that they are all orthogonal. (Proof in the textbook)\n",
    "\n",
    "We know that if $A$ has $n$ independent eigenvectors, then we can write $A = S \\Lambda S^{-1}$. Now if $A$ is symmetric, we can choose the eigenvectors such that the columns of $S$ are orthonormal. We represent this $S$ by a special letter $Q$. Because of the orthonormality, $QQ^T = I$ and $Q^{-1} = Q^T$. Thus, $A = Q \\Lambda Q^T$ in case $A$ is symmetric. This is called the spectral theorem; spectrum means the eigenvalues of the matrix.  \n",
    "\n",
    "If $A$ is symmetric we can write,\n",
    "$$A = Q\\Lambda Q^T = \\mmat{q_1&q_2&\\dots}\\mmat{\\lambda_1&&\\\\&\\lambda_2&\\\\&&\\ddots}\\mmat{q^T_1\\\\q^T_2\\\\\\vdots} = \\lambda_1q_1q^T_1 + \\lambda_2q_2q^T_2 + \\dots$$  \n",
    "\n",
    "Remember, that if we want to find the projection of $\\vec{p}$ on $\\vec{a}$, we multiply $\\vec{p}$ by the matrix $\\frac{aa^T}{a^Ta}$. $q_1q^T_1$ is the projection matrix for $q_1$ as it is a unit vector. Also, all the columns of a projection matrix are scaled versions of the same vector. Thus a projection matrix is a rank one matrix! And from the equation above, every symmetric matrix can be decomposed as a linear combination of perpendicular projection\n",
    "(rank one) matrix! This is another way of thinking about the Spectral Theorem.\n",
    "\n",
    "**Positive Definiteness**  \n",
    "\n",
    "Number of positive/negative eigenvalues for symmetric matrices can be determined from the signs of the\n",
    "pivots. The number of positive eigenvalues is the same as the number of positive pivots.  \n",
    "\n",
    "A positive definite matrix is a symmetric matrix (denoted by PSD). If symmetric matrices are “good” matrices, PSD are “excellent”. It is a symmetric matrix with all eigenvalues are positive. Of course, all the pivots are positive.\n",
    "\n",
    "So, for 2 × 2 matrices, PSD matrices always have positive determinants and positive trace. However, this is not a sufficient condition for positive definiteness, as can be demonstrated by the matrix $\\mmat{-1&0\\\\0&-3}$.  \n",
    "\n",
    "We state that a matrix is positive definite if all its subdeterminants are positive; the determinants of\n",
    "submatrices formed by taking a m × m submatrix from the left top corner. Also, remember that this a necessary condition for Positive Definiteness, not a sufficient one. See [this](https://math.stackexchange.com/questions/235170/necessary-condition-for-positive-semidefiniteness-is-it-sufficient) for more information.\n",
    "\n",
    "To summarize, for a PSD matrix:  \n",
    "• All eigenvalues are positive  \n",
    "• All pivots are positive  \n",
    "• All subdeterminants are positive  \n",
    "\n",
    "### Lecture 27 (Positive Definite Matrix; Minimas)  \n",
    "\n",
    "So we have a symmetric matrix $A = \\mmat{a&b\\\\b&c}$. Is this positive definite? The following tests (individually) can tell if a symmetric matrix (symmetry is given) is also a positive definite matrix:\n",
    "1. Positive eigenvalue test.\n",
    "2. Positive principal submatrix test.\n",
    "3. Positive pivots test.\n",
    "\n",
    "We add another test to this collection of tests. It goes as follows:\n",
    "$$x^TAx > 0, \\forall x>0$$  \n",
    "Most texts state the above property as the definition of positive definite matrices. We will be exploring this property in the current lecture. Also, if all the positiveness in all the above tests is swapped by nonnegativeness, we get a positive **semidefinite** matrix. For example, for $A = \\mmat{2&6\\\\6&a}$, if $a > 18$ we get a positive definite matrix, but if $a=18$, then the matrix is positive semidefinite. For $x=\\mmat{x_1\\\\x_2}$, $$x^TAx = 2x_1^2 + 12x_1x_2 + 18x_2^2$$\n",
    "which is a pure quadratic equation (each term has degree 2) in $x_1,x_2$. If this quadratic equation is $>0,\\forall x > 0$, then $A$ is positive definite, and if the quadratic equation is $\\geq0, \\forall x>0$, then $A$ is positive semidefinite.  \n",
    "\n",
    "Let us generalize our discussion. For the matrix $A = \\mmat{a&b\\\\b&c}$, $\\vec{x}^TA\\vec{x}$ is of the form $f(x,y) = ax^2 + 2bxy + cy^2$, where $x$ and $y$ stand for XY co-ordinates and $\\vec{x}$ is any 2-D vector. Now, a little revision on Calculus. The following conditions hold for a Maxima, Minima and Saddle points:\n",
    "1. Maxima : All first-order derivatives are 0, second-order derivatives are < 0.\n",
    "2. Minima : All first-order derivatives are 0, second-order derivatives are > 0.\n",
    "3. Saddle Points : All first-order derivatives are 0, second-order derivatives along some lines > 0 and along other lines < 0.\n",
    "\n",
    "For positive definite matrices, $f(x,y)$ will be of the form of sum of squares and thus $f(x,y) = 0$ at $(x,y) = (0,0)$ and positive at other points. Thus (0,0) will be a minima.\n",
    "\n",
    "**Side-Note**: For Minima of $f(x,y)$, all the second-order derivatives need to be greater than zero. But how do we verify that the second-order derivatives in all directions (along all lines) are > 0? $f_{xx}>0$ & $f_{yy}>0$ isn't sufficient. They need to be positive enough. If we have $\\mmat{f_{xx} & f_{xy}\\\\f_{yx}&f_{yy}}>0$ then all second-order derivates are more than zero, and we have a Minima!\n",
    "\n",
    "### Lecture 28 (Similar Matrices)  \n",
    "\n",
    "First we continue our discussion of Positive Definite (PSD) Matrices. If A is PSD and B is PSD, can we say anything about A+B? Yes! A+B is also Positive Definite. How? $$x^T(A+B)x = x^TAx + x^TBx > 0\\;\\; (x^TAx >0\\,\\&\\,x^TBx > 0)$$  \n",
    "\n",
    "Now suppose $A$ is a mxn matrix. Will $A^TA$ be positive definite? Surprisingly, YES!\n",
    "$$x^TA^TAx=(Ax)^T(Ax)=\\Vert{Ax}\\Vert^2 > 0$$  \n",
    "\n",
    "$\\Vert{Ax}\\Vert^2 > 0$ is true only if the null space of $A$ only has $\\vec{0}$. This is true if $rank(A) = n$  \n",
    "\n",
    "Finally we get started on Similar Matrices. Two n×n matrices $A$ and $B$ are similar, if for some invertible $M$, such that $A = MBM^{−1}$. Example: $A$ is similar to $Λ$ because $Λ = S^{−1}AS$. Why do we call them similar? They have the same eigenvalues. How do we prove it? Let $Ax = λx$. Then $Ax = MBM^{−1}x = λx \\iff BM^{−1}x = λM^{−1}x$. Thus, $M^{−1}x$ is an eigenvector of B with the same eigenvalue.  \n",
    "\n",
    "We see that similar matrices have the same eigenvalues. Also, if two matrices have the same n distinct eigenvalues, they’ll be similar to the same diagonal matrix. If $A$ has a full set of eigenvectors we can create its eigenvector matrix $S$ and write $S^{−1} AS$ = Λ. So $A$ is similar to $Λ$ (choosing $M$ to be $S$). To sum up, \"similarity\" divides matrices into groups with identical set of eigenvalues. The most “preferable” of them, obviously, are diagonal matrices. They have trivial eigenvectors.  \n",
    "\n",
    "What if all the n eigenvalues are not distinct? What if two or more eigenvalues are the same? Then the matrix might not be diagonalizable - it might not have a full set of independent eigenvectors. What happens now? For more information on this case and on Jordan Matrix read the [lecture summary](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/similar-matrices-and-jordan-form/MIT18_06SCF11_Ses3.4sum.pdf). Jordan Matrix can be skipped over.  \n",
    "\n",
    "### Lecture 29 SVD :)  \n",
    "\n",
    "Singular Value Decomposition is the process of breaking down a matrix $A$ into a product of three matrices $U, \\Sigma, V^T$, such that $U$ and $V^T$ are orthogonal and $\\Sigma$ is diagonal. We have already seen how to do it in case $A$ is symmetric $(A = Q\\Lambda Q^T)$. This is a special case where $U = V = Q$. We've also seen $A=S\\Lambda S^{-1}$, when $A$ is square and has eigenvectors spanning the entire n-dimensions. But here $S$ is not necessarily orthogonal. SVD applies to all matrices $A$ (even rectangular ones), and decomposes it into two orthogonal and one diagonal matrix.  \n",
    "\n",
    "If $A$ is a mxn matrix then the row space (defined differently here, as the space of vectors that can be multiplied to $A$) is $\\mathbb{R}^n$ and the column space is a subset of $\\mathbb{R}^m$ and $A$ transforms a vector from row space to column space. We want to find an **orthonormal** basis ($V$) in row space that transforms into an orthonormal basis ($U$) in the column space, such that $AV = U\\Sigma$.\n",
    "\n",
    "Side-note-  \n",
    "<ol>\n",
    "<li>Remember that the Rank-Nullity theorem states that:\n",
    "For any m × n matrix $A$,\n",
    "rank($A$) + nullity($A$) = n.  \n",
    "Thus if $V$ is the orthonormal basis of the row-space (not the conventional definition where row space is linear combination of rows), then $r = rank(A)$ columns of $V$ will map to non-zero vectors and other $n-r$ would map to $\\vec{0}$ in the column space. Thus $\\Sigma$ will have r non-zero diagonal elements and (n-r) zero diagonal elements.</li>\n",
    "    <li>Also, note that $V$ is orthonormal, so $V^{-1}=V^T$.\n",
    "    </li>\n",
    "</ol> \n",
    "    \n",
    "\n",
    "After this, you can refer to this [wonderful note](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf) summarizing the entire SVD lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

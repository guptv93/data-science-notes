{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\mmat}[1]{\\begin{bmatrix} #1 \\end{bmatrix}}\n",
    "\\newcommand{\\vac}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\real}[1]{\\mathbb{#1}}\n",
    "\\newcommand{\\C}{\\mathcal{C}}\n",
    "$\n",
    "\n",
    "## SVD \n",
    "\n",
    "First get an intuitive understanding of Singular Value Decomposition from this answer on [Quora](https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD). Basically SVD, or Eigenvalue Decomposition of a Symmetric Matrix A gives us orthogonal eigenvectors.\n",
    "$$A = Q\\Lambda Q^T = \\mmat{q_1&q_2&\\dots}\\mmat{\\lambda_1&&\\\\&\\lambda_2&\\\\&&\\ddots}\\mmat{q^T_1\\\\q^T_2\\\\\\vdots} = \\lambda_1q_1q^T_1 + \\lambda_2q_2q^T_2 + \\dots$$  \n",
    "\n",
    "Here, $\\lambda_iq_iq^T_i$ are single rank matricies where each column vector is a scaled version of the vector $q_i$. If the matrix is rank $r$ then there will be r such $q_i$(s). Now, suppose we want to find a reduced rank approximation $\\hat{A}$ of $A$. For this we arrange all the Spectral terms $\\lambda_iq_iq^T_i$ in descending order of $\\lambda_i$ and then let the last $\\lambda_rq_rq^T_r$ drop out of Spectral summation. This way $\\hat{A}x$ will now be the projection of $Ax$ on the reduced dimensionality column space of $\\hat{A}$ and the AVERAGE L2Norm distance between $\\hat{A}x$ and $Ax$ will be the least for a randomly picked $x$. Thus $\\lambda_iq_iq^T_i$ when arranged in descending order can be thought of as giving us principal components of the matrix $A$.  \n",
    "\n",
    "This shows us how SVD (also known as spectral decomposition) helps us break a symmetric matrix into a spectrum of single rank matrices. If we choose most of the higher spectrum, then the projections of $\\mathbf{x}$ will be such that we loose least information (variance). The information lost is depicted by drawing a perpendicular from $\\mathbf{x}$ to the lower rank column space. SVD is a way to reduce the rank of the matrix in a way that the transformation by the approximation matrix is still least distant (L2 Norm wise) from the original transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA  \n",
    "\n",
    "First see Andrew NG's videos on PCA to understand the purpose of PCA and how PCA differs from Linear Regression. Few bullet points:\n",
    "* PCA is reduce correlation between **independent** variables\n",
    "* PCA tried to minimize projection length between the original vector and its reduced dimensionality projection. Linear Regression tries to minimize the vertical distance between a dependent variable and its prediction.  \n",
    "\n",
    "Now let us get into the technicality of PCA. Assume that we start with a data set that is represented in terms of an $m\\,\\times\\,n$ matrix $\\vac{X}$, where the $n$ columns are the samples (e.g. observations) and the $m$ rows are the variables. We wish to linearly transform this matrix, $\\vac{X}$ into another matrix, $\\vac{Y}$, also of dimension $m × n$, so that for some $m × m$ matrix, $\\vac{P}$,  \n",
    "$$\\vac{Y=PX}$$\n",
    "\n",
    "Now if $\\vac{X}$ is zero-centered (mean is zero for each variable), then the covariance matrix of $\\vac{X}$ is given by \n",
    "$$\\vac{C_Y = \\frac{1}{n-1}XX^T}$$  \n",
    "\n",
    "Covariance can be considered to be a measure of how well correlated two variables are. The PCA method makes the fundamental assumption that the variables in the transformed matrix should be as uncorrelated as possible. This is equivalent to saying that the covariances of different variables in the matrix CY, should be as close to zero as possible (covariance matrices are always positive definite or positive semi-definite). Conversely, large variance\n",
    "values interest us, since they correspond to interesting dynamics in the system (small variances may well be noise). We therefore have the following requirements for constructing\n",
    "the covariance matrix, $\\vac{C_Y}$:\n",
    "1. Maximise the signal, measured by variance (maximise the diagonal entries)\n",
    "2. Minimise the covariance between variables (minimise the off-diagonal entries)  \n",
    "\n",
    "We thus come to the conclusion that since the minimum possible covariance is zero, we are\n",
    "seeking a diagonal matrix, $\\vac{C_Y}$. If we can choose the transformation matrix, $\\vac{P}$ in such a\n",
    "way that $\\vac{C_Y}$ is diagonal, then we will have achieved our objective.  \n",
    "\n",
    "Remember that linear transformations don't change the origin, so $\\vac{Y}$ is also zero centered. Therefore, \n",
    "$$\\vac{ C_Y = \\frac{1}{n-1}YY^T = \\frac{1}{n-1}(PX)(PX)^T = \\frac{1}{n-1}PXX^TP^T = \\frac{1}{n-1}PXX^TP^T = \\frac{1}{n-1}PC_XP^T }$$   \n",
    "\n",
    "We know that $\\vac{C_X}$ is a symmetric sqaure matrix and so it can be written as $\\vac{Q\\Lambda Q^T}$, where $\\vac{Q}$ is an orthonormal matrix with eigenvectors of $\\vac{C_X}$ as the column vectors. If we take $\\vac{P=Q^T}$, then $\\vac{C_Y}$ is a scaled version of diagonal matrix $\\vac{\\Lambda}$. Thus the linear transformation that we are looking for is $\\vac{Y=Q^TX}$, where $\\vac{Q}$ is the eigenvector matrix of $\\vac{C_X}$. Thus we take projections of $\\vac{x_i}$s onto the eigenvectors of $\\vac{C_X}$.  \n",
    "\n",
    "Last thing to note is that if $\\vac{\\Lambda}$ is arranged in descending order of diagonal values, then we can drop the variables corresponding to the last few columns of $\\vac{C_Y}$ as they show the least variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\C}{\\mathcal{C}}\n",
    "$\n",
    "# Essence of Linear Algebra - I\n",
    "\n",
    "*This is meant to accompany each corresponding video in the Essence of Linear Algebra Series*  \n",
    "\n",
    "\n",
    "### What are Vectors?\n",
    "\n",
    "Is it a list of numbers (CS student perspective)? Is it an arrow in space with a length and direction and that can be moved around in space (physics student perspective)? We define it with a Maths perspective. For a maths student, it is anything for which the two operations, vector addition and scalar multiplication, make sense.\n",
    "\n",
    "The video also goes over vector addition and multiplication.  \n",
    "\n",
    "\n",
    "### Linear Combinations, Span, and Basis Vectors\n",
    "\n",
    "Linear Combination : Scaling and adding two vectors.\n",
    "\n",
    "Span : All vectors that can be obtained by taking linear combinations of a set of vectors is the span of the particular set of vectors.\n",
    "\n",
    "Vector Space : A set of vectors in which addition and scalar multiplication are closed operations. All spans constitute a vector space.\n",
    "\n",
    "Linearly Independent : Suppose we have a set of vectors. If adding another vector doesn’t add anything to the span of the original vectors, then the added vector is a linear combination of the original vectors. In this case, the set of vectors (original plus last added) are Linearly Dependent. On the other hand, if the newly added vector adds more vectors to the span of the previous set, then the new vector is said to add a Dimension to the span.\n",
    "\n",
    "Basis Vectors : The basis of a vector space is a set of linearly independent vectors that span the full space.\n",
    "\n",
    "Unit Vector : A vector with length 1. Unit vectors in X and Y directions are generally denoted using the symbols $\\hat{i}, \\hat{j}$.\n",
    "\n",
    "Dimension : In the definition of Linear Independence, if the newly added vector adds more vectors to the span, then the new vector is said to add a dimension to the span. The number of vectors in the basis of a vector space is the dimension of the vector space. Now we know that each vector space can have infinitely many basis sets. Do all of these basis sets have the same number of vectors? Yes! [Proof: Any subspace basis has same number of elements (video) |Khan Academy](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/null-column-space/v/proof-any-subspace-basis-has-same-number-of-elements)  \n",
    "\n",
    "\n",
    "### Linear Transformations and Matrices\n",
    "\n",
    "What is a transformation? It is a function that maps every input vector of a vector space to an output vector. You can visualize a transformation as taking every vector in a vector space and *moving* it to a new vector in the same vector space. Visualizing so many vectors moving all at once can be a bit tricky. For simplicity, we can represent each vector by a point sitting at the tip, and then visualize the points (tips) moving from one location to the other, instead of the entire vectors. Further, instead of looking at the movement of all the points, it is more intuitive to only look at the points on the gridlines.\n",
    "\n",
    "Linear Algebra is limited to the study of *Linear* Transformations. What are *Linear* Transformations? You can think of a linear transformation as arbitrarily moving unit vectors to different locations, and then the locations of all other vectors $\\vec{x}$ are determined as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{x} &= a\\vec{i} + b\\vec{j} \\\\\n",
    "f(\\vec{x}) &= af(\\vec{i}) + bf(\\vec{j}) & f\\text{ is a linear transformation of vector space}\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "Because of the above mentioned property, it is obvious that the grid lines remain (firstly linear,) parallel and equidistant from each other during a linear transformation. Why? Because suppose $\\hat{i}, \\hat{j}$ are moved to $\\hat{i'}, \\hat{j'}$, then the new distance between points (1,0) and (2,0) will be equal to that between points (2,0) and (3,0) (both being equal to $\\|\\hat{i'}\\|$) and so on. A transformation of a vector space is a linear transformation if and only if the new grid lines are still parallel and equidistant from ech other.\n",
    "\n",
    "A matrix is a horizontal list of vectors. Matrix helps us concisely denote a linear transformation of space. It lists the new locations of the unit vectors after the transformation. Multiplying a vector with a matrix gives the transformed vector.\n",
    "\n",
    "Multiplying a vector with a matrix can be thought of denoting a linear combination of the column vectors of the matrix. Each column vector is first scaled by the corresponding row entry in the vector and then all these scaled column vectors are added. Thus, if $\\vec{x}$ started off as a linear combination of the unit vectors $\\hat{i}, \\hat{j}$, then after the transformation it ends up being the same linear combination of the transformed unit vectors $\\hat{i'}, \\hat{j'}$.  \n",
    "\n",
    "Another way of thinking about Linear Transformation (apart from moving all vectors in the vector space) is in terms of change of the co-ordinate system. Instead of moving each point, we just give each point different coordinates. Multiplying by the transformation matrix takes us from the new coordinate system to the old coordinate system. This is discussed further under the topic \"Change of Basis\".\n",
    "\n",
    "### Matrix Multiplication as Composition\n",
    "\n",
    "Many a times we want to sequentially apply multiple transformations to a vector spac, one after the other. This is called a Composition of Linear Transformations. It is easy to understand why a composition of linear transformations will be a linear transformation all in itself (the gridlines will still be parallel and the origin would not have moved). Also, if $g$ and $f$ are the first and second linear transformations, then it is obvious that:\n",
    "$$(f\\circ g)(\\vec{x}) = a(f\\circ g)({\\hat{i}}) + b(f\\circ g)({\\hat{j}})$$\n",
    "\n",
    "Thus the Composite Linear Transformation ($f\\circ g$) can be expressed in the matrix form.  \n",
    "\n",
    "How do we apply two consecutive transformations, first $g = [\\vec{g_1}, \\vec{g_2}]$ and second $f = [\\vec{f_1}, \\vec{f_2}]$? We first take the unit vectors $\\hat{i}, \\hat{j}$ to $\\hat{i'} = \\vec{g_1}, \\hat{j'} = \\vec{g_2}$. We note where each $\\vec{x}$ lands. Let us call it $\\vec{x'}$. To apply the second transformation, we move $\\hat{i}, \\hat{j}$ to $\\vec{f_1}, \\vec{f_2}$. Notice that $\\hat{i'}, \\hat{j'}$ are not the vectors landing on $\\vec{f_1}, \\vec{f_2}$; $\\hat{i}, \\hat{j}$ are! Why? The co-ordinates of $\\vec{x'}$ are with respect to the [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra)) $[\\hat{i}, \\hat{j}]$. Therefore when a second transformation is applied, the unit vectors $[\\hat{i}, \\hat{j}]$ move to the column vectors of the second matrix.  \n",
    "\n",
    "Let the landing location of $\\vec{x'}$ after the second transformation be $\\vec{x''}$. Then $\\vec{x''} = f(g(\\vec{x}))$. If $\\hat{i''} = f(g(\\hat{i}))$ and $\\hat{j''} = f(g(\\hat{j}))$, then $f\\circ g = [\\hat{i''}, \\hat{j''}]$. This also gives us the defining formula of Matrix Product:\n",
    "\n",
    "$$[\\vec{f_1}, \\vec{f_2}][\\vec{g_1}, \\vec{g_2}] = [\\hat{i''}, \\hat{j''}]$$ \n",
    "\n",
    "\n",
    "### Three Dimensional Linear Transformations\n",
    "\n",
    "Here we discuss the case where instead of 2-D flat plane we have 3-D space. We discuss linear transformations in 3-D.\n",
    "\n",
    "\n",
    "### The Determinant\n",
    "\n",
    "A Determinant represents the number by which the unit square/cube is squished or stretched after a 2-D/3-D transformation. The value of the determinant is (-ve) if the orientation of the grid has been flipped by the transformation. Negative value of the determinant can be visualized as the area of unit square slowly reducing to zero and then increasing on the other side of orientation of X-axis and Y-axis.  \n",
    "\n",
    "\n",
    "### Inverse Matrices, Column Space and Null Space\n",
    "\n",
    "Matrices are mostly used to solve systems of linear equations. The process to do this is Gaussian Elimination and Row Echelon Form which we discuss in the coming note. Here we discuss the intuition behind this method of solving linear equations. In this section we consider only square matrices (number of equations is equal to the number of unknowns). \n",
    "\n",
    "Here we consider only square matrices. The case of squishing the vector space is represented by determinant of matrix being 0.  If a matrix multiplication doesn’t squish the space, then another matrix transformation can bring each vector to exactly where it was before the transformation. This new matrix is known as the inverse of previous matrix. \n",
    "\n",
    "The case where the transformation squishes the space (the determinant is zero) is more complicated. No function can un-squish the space.  We may still get a solution but for that we need to be lucky. The transformed vector needs to lie in the squished space.\n",
    "\n",
    "Rank is defined as the dimension of the column span. It is obvious that it is the number of independent columns in the transformation matrix. Our next note describes what pivot columns are and how the rank is equal to the number of pivot columns.\n",
    "\n",
    "The video also helps get an intuitive understanding of why rank(column space) + rank(null space) = n.  \n",
    "\n",
    "\n",
    "### Nonsquare matrices as transformations between dimensions\n",
    "\n",
    "A 2x2 matrix transforms from a 2-D co-ordinate system to a 2-D co-ordinate system. Similarly for a 3x3 matrix. What about non-square matrices? Non-square matrices are transformations between dimensions. A 2x3 matrix is a transformation that takes in a vector in 3-D and gives out a vector in 2-D. Thus it reduces the dimension by 1. Observe that this is not the same as a 3x3 matrix that reduces the space to a flat plane. The co-ordinates of the output vector are still in 3-D, though the basis of the column space has only two vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

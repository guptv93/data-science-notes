{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\mmat}[1]{\\begin{bmatrix} #1 \\end{bmatrix}}\n",
    "\\newcommand{\\C}{\\mathcal{C}}\n",
    "$\n",
    "\n",
    "# Orthogonality and Projections\n",
    "\n",
    "### Lecture 14 (Definition of Orthogonality)\n",
    "What does it mean for two subspaces/vectors/basis to be orthogonal?  \n",
    "\n",
    "**Vector Orthogonality**: Two vectors $x$ and $y$ are orthogonal iff $\\vec{x}\\cdot\\vec{y} = 0$ or in other words, $x^Ty=0$.\n",
    "\n",
    "**Subspace orthogonality**: Two subspace $S$ and $T$ are orthogonal to each other, when every vector in $S$ is orthogonal to every vector in $T$.  \n",
    "\n",
    "Are two perpendicular planes in $R^3$ orthogonal to each other? No. Vectors in the intersection set are not orthogonal. Two orthogonal subspaces can only have $\\vec{0}$ in their intersection set. \n",
    "\n",
    "Null space & Row space of a $m \\times n$ matrix are both spaces in $R^n$ and are orthogonal to each other. Why? $x \\in N(A)$ iff\n",
    "$$Ax = \\mmat{r_1 \\cdotp x\\\\ \\vdots \\\\r_n \\cdotp x} = \\mmat{0 \\\\ \\vdots \\\\ 0}$$  \n",
    "\n",
    "Null space & Row space of a matrix are not only orthogonal, but are also orthogonal complements of each other. Nullspace contains all vectors orthogonal to the row space.  \n",
    "\n",
    "**Orthogonal Complement**: If a subspace $V$ contains all the orthogonal vectors to another subspace $T$, then $V$ is called orthogonal complement of $T$, denoted by $T^\\bot$.\n",
    "\n",
    "### Lecture 15 (Projections)\n",
    "We want to find the projection of $\\vec{b}$ on $\\vec{a}$. Let $\\vec{p} = x \\vec{a}$ be the projection vector.\n",
    "\n",
    "Then,  \n",
    "$\\qquad \\vec{a}\\cdot(\\vec{b} - \\vec{p}) = a^T(\\vec{b} - x\\vec{a}) = a^Tb - a^Tax = 0$  \n",
    "$\\iff x = \\frac{a^Tb}{a^Ta} = \\frac{\\vec{a}\\cdot\\vec{b}}{{\\|a\\|}^2}$  \n",
    "$\\iff x\\vec{a} = \\vec{p} = a\\frac{a^Tb}{a^Ta} = \\frac{aa^T}{a^Ta}b$  \n",
    "\n",
    "The matrix $\\frac{aa^T}{a^Ta}$ is called the Projection Matrix $P$.  \n",
    "<br/>\n",
    "\n",
    "Suppose we want to solve $Ax = b$. Then $b$ has to be in column space of $A$. If $b$ is not in the column space, then we can solve for $p$ such that $p \\in \\C(A)$ and $p$ is the closest point to $b$ in $\\C(A)$. By Pythagoras Theorem, this $\\vec{p}$ has to be the projection of $\\vec{b}$ on $\\C(A)$.\n",
    "\n",
    "For some $\\hat{x}, A\\hat{x} = p$. $p$ is the projection of $b$ on the column space of $A$. Therefore,  \n",
    "$\\qquad A^T(b - A\\hat{x}) = 0$  \n",
    "$\\implies \\hat{x} = (A^TA)^{-1}A^TB$  \n",
    "$\\implies p = A\\hat{x} = A(A^TA)^{-1}A^Tb$  \n",
    "\n",
    "$A(A^TA)^{-1}A^T$ is called the projection matrix $P$ of $A$. If $A$ is an invertible square matrix, then $P$ becomes $I$ (identity matrix). This is because $b$ will always be in the column space of $A$ (whatever the vector $b$ is).\n",
    "\n",
    "### Lecture 16 (Regression)\n",
    "<img src=\"./images/L16-1.png\"/>\n",
    "<img src=\"./images/L16-2.png\"/>\n",
    "\n",
    "### Lecture 17 (Orthonormal Basis)\n",
    "\n",
    "A set of orthonormal vectors $\\{q_1, q_2,\\dots\\}$ is a set of unit vectors where every pair is perpendicular. We can\n",
    "write this with a matrix $Q = \\mmat{q_1 & q_2 & \\dots}$, where $q_i$'s are column vectors. The above requirement can be written as $Q^TQ=I$. This matrix $Q$ is called a orthonormal matrix. What happens when Q is square? Since $Q^TQ=I$, we have $Q^{-1} = Q^T$.  \n",
    "\n",
    "If $Q$ is orthonormal, then the Projection Matrix $$P = Q(Q^TQ)^{-1}Q^T = Q(I)^{-1}Q^T = QQ^T$$ Remember that $QQ^T$ is not equal to the Identity Matrix.\n",
    "\n",
    "But why is finding an Orthonormal Basis **soo** important? The reason is as follows:  \n",
    "Suppose $\\vec{b}$ is any vector, and $Q$ is an orthonormal basis of some vector space.  \n",
    "$\\qquad Q^T(\\vec{b} - Q\\hat{x}) = 0$  \n",
    "$\\implies Q^Tb = Q^TQ\\hat{x}$  \n",
    "$\\implies \\hat{x}=Q^Tb$  \n",
    "$\\implies \\hat{x}_i = (q_i)^Tb$  \n",
    "\n",
    "Thus if all columns of $Q$ are orthonormal, then calculating projection of $\\vec{a}$ on $\\C(Q)$ is the same as combining the projections of $\\vec{a}$ on each of the basis vector individually. This equation is very easy to compute. With orthonormal basis, calculating projections becomes almost trivial.  \n",
    "\n",
    "How do you calculate Orthonormal Basis from a given basis? **Gram-Schmidt Algorithm** does that for you.  \n",
    "<ul>\n",
    "    <li> Take an arbitrary vector from the given basis, and normalize it. Include it in the result set.</li>\n",
    "    <li> For each other vector $v$ in given basis,\n",
    "        <ul>\n",
    "            <li>  For each vector $u$ in the result set, subtract the projection of $v$ onto $u$ from $v$: $v = v - (u^Tv)\\cdot u $</li>\n",
    "            <li> Normalize the resulting $v$ and include it in the result set.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

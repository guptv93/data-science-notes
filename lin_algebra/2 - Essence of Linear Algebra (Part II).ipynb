{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\mmat}[1]{\\begin{bmatrix} #1 \\end{bmatrix}}\n",
    "\\newcommand{\\C}{\\mathcal{C}}\n",
    "$\n",
    "\n",
    "# Essence of Linear Algebra II\n",
    "\n",
    "*This is meant to accompany each corresponding video in the Essence of Linear Algebra Series*  \n",
    "\n",
    "### Dot Products and Duality\n",
    "(We finally talk about projections!)\n",
    "\n",
    "First, we study the meaning of a dot product. Dot product of $\\vec{u}$ and $\\vec{v}$ means the projection of $\\vec{u}$ on $\\vec{v}$ into the length of $\\vec{v}$. From the definition, it feels odd that dot product of $\\vec{u}$ and $\\vec{v}$ is the same as dot product of $\\vec{v}$ and $\\vec{u}$. The video gives the intuition to why $\\vec{u}\\cdot\\vec{v} = \\vec{v}\\cdot\\vec{u}$.  \n",
    "\n",
    "Now for 2D vectors, $\\vec{u} \\cdot \\vec{v} = u_xv_x + u_yv_y$. To understand the derivation of this formula, we need to understand duality. The Duality property has been stated below. See the video for its explanation.\n",
    "\n",
    "**Duality**  \n",
    "Any vector $\\vec{u} = \\mmat{u_x \\\\ u_y}$ can be looked at as a matrix $\\mmat{u_x&u_y}$ which transforms 2D space into 1D space.  Projection of $\\vec{v}$ on $\\vec{u}$ can be thought of as the linear transformation of $\\vec{v}$ by the matrix $u^T$.    \n",
    "\n",
    "This property of Duality, of seeing every projection as a linear transformation from 2D to 1D, is what leads to the derivation of dot product as elementwise vector multiplication $(\\vec{u} \\cdot \\vec{v} = u_xv_x + u_yv_y)$.\n",
    "\n",
    "\n",
    "### Cross Products\n",
    "The cross product of two vectors $\\vec{u},\\vec{v}$ (denoted as $\\vec{u}\\times\\vec{v}$) is another vector that is perpendicular to the plane formed by $\\vec{u}$ and $\\vec{v}$ and whose length is equal to the area of the parallelogram formed by $\\vec{u}$ and $\\vec{v}$.  \n",
    "\n",
    "$\\|\\vec{u}\\times\\vec{v}\\| = det(\\mmat{\\vec{u}&\\vec{v}})$  \n",
    "$\\vec{u}\\times\\vec{v} = - \\vec{v}\\times\\vec{u}$  \n",
    "<br/>\n",
    "\n",
    "*The next two videos \"cross products in the light of linear transformations\" & \"cramer's rule, explained geometrically\", fall outside the scope of essence of linear algebra. Go through them, only if needed. Otherwise knowing how to calculate the cross product and how to solve equations using Cramer's rule suffices.*  \n",
    "<br/>\n",
    "\n",
    "### Change of Basis\n",
    "We all share the same Space and look at the same vectors. However we use different languages to describe the vectors. These languages are called co-ordinate systems. All the languages agree on only one thing. The location of the origin $(0,0)$, where a vector lands if you scale if by $0$. The default co-ordinate system that we use is represented by $\\C$. $\\C$ has unit vectors $\\hat{i},\\hat{j}$ (known as the basis of the co-ordinate system), written as $\\C=<\\hat{i},\\hat{j}>$.  \n",
    "\n",
    "How do the co-ordinates of a vector change when we change the basis vectors (co-ordinate system)? There is nothing confusing about it unlike what is said in the videos. Suppose we have a matrix $A$ that transforms $\\C$ into $\\C'$. Multiplying $\\vec{x}$ (in $\\C$) with $A$, we get $\\vec{b}$ (in $\\C$). This $\\vec{b}$ (in $\\C$) is the same as $\\vec{x}$ (in $\\C'$). Thus, to convert the coordinates of any $\\vec{x}$ (in $\\C'$) to the co-ordinate system $\\C$, we multiply $\\vec{x}$ with the matrix A (such that the columns of matrix A represent the new basis vectors after transformation).\n",
    "\n",
    "Thus, if I want to use a co-ordinate system with $\\mmat{\\vec{u_x}&\\vec{u_y}&\\vec{u_z}}$ as basis vectors, then the co-ordinates of $\\vec{x}$ in the new co-ordinate system are given by $\\mmat{\\vec{u_x}&\\vec{u_y}&\\vec{u_z}}^{-1}\\vec{x}$.  \n",
    "\n",
    "Another way of looking at this is $\\mmat{\\vec{u_x}&\\vec{u_y}&\\vec{u_z}}\\mmat{\\vec{u_x}&\\vec{u_y}&\\vec{u_z}}^{-1}\\vec{x} = \\vec{x}$. Thus, $\\mmat{\\vec{u_x}&\\vec{u_y}&\\vec{u_z}}^{-1}\\vec{x}$ is the representation of $\\vec{x}$ when basis vectors are $\\vec{u_x},\\,\\vec{u_y},\\,\\vec{u_z}$.\n",
    "\n",
    "### Eigenvectors and Eigenvalues\n",
    "There might be some vectors during a linear transformation, that only get stretched or squished (and don't change their direction). Scalar multiples of such vectors will also have the same property of not changing their direction. Such vectors are called the eigenvectors of a transformation. The factor by which an eigenvector is stretched or squished is called the eigenvalue of that eigenvector.  \n",
    "\n",
    "If $\\vec{v}$ is the eigenvector for a transformation A, and the corresponding eigenvalue is $\\lambda$, then \n",
    "$$A\\vec{v} = \\lambda\\vec{v}$$\n",
    "$$\\implies(A - I\\lambda)\\vec{v} = \\vec{0}$$\n",
    "\n",
    "If $det(A - \\lambda I) \\neq 0$, then $\\vec{v} = \\vec{0}$ is the only solution of the above equation. Therefore eigenvectors exist only if $det(A - \\lambda I) = 0$.  \n",
    "\n",
    "Thus, the method of finding eigenvectors is as follows:\n",
    "<ul>\n",
    "    <li> Find $\\lambda$ by solving the equation $det(A - \\lambda I) = 0$.</li>\n",
    "    <li> Sequentially plug in each value of $\\lambda$ and derive the matrix $(A - I\\lambda)$.</li>\n",
    "    <li> Solve $(A - I\\lambda)\\vec{v} = \\vec{0}$ to find $\\vec{v}.$\n",
    "</ul>  \n",
    "\n",
    "Remember a linear transformation doesn't always have an eigenvector. For example, the rotation operation doesn't have any eigenvectors. Shear transformation $M = \\mmat{1&1\\\\0&1}$ has single eigenvalue(1). A linear transformation that scales every vector by 2, has a single eigenvalue(2), but has infinitely many eigenvectors in each direction.   \n",
    "\n",
    "If the basis vectors are themselves eigenvectors of some transformation, then the transformation is represented by a diagonal matrix. In a diagonal matrix, only the elements of the principal diagonal are non-zero. We see that calculating higher powers of a diagonal matrix is trivial. But, we'll be rarely so lucky to have our basis vectors also be the eigen-vectors. How do we calculate higher powers for matrices whose eigenvectors are not the basis vectors? We can change our co-ordinate system so that the eigenvectors of the matrix are the basis vectors of our new co-ordinate system. See the video for more details. Before that you might also need to revise how you represent the same linear transformation (like a 90$^\\circ$ rotation) in different co-ordinate systems from [previous video](https://youtu.be/P2LTAUO1TdA?t=537)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Eigendecomposition\n",
    " *This is not a part of the Essence Videos. The reference used in [Deep Learning Series - Eigendecomposition](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/)*\n",
    " \n",
    "The eigendecomposition is one form of matrix decomposition. Decomposing a matrix means that we want to find a product of matrices that is equal to the initial matrix. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues. It is essential to have a good understanding of Eigenvectors and Eigenvalues before getting into eigendecomposition.  \n",
    "\n",
    "The pre-requisite for eigendecomposition of a matrix $A$ is that $A$ should have enough eigenvectors to span the entire space. In that case, we can construct a matrix $V$, such that $V$ is a square matrix with each column being an eigenvector of $A$ and the column space spanning full space. Also let $diag(\\lambda)$ represent the diagonal matrix whose each diagonal value is the eigenvalue of the corresponding column of $V$. In such a case, \n",
    "$$A = V\\cdot diag(\\lambda)\\cdot V^{-1}$$  \n",
    "\n",
    "In the video on \"Eigenvectors and Eigenvalues\", we saw how $V^{-1}\\cdot A \\cdot V = diag(\\lambda)$. This implies the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

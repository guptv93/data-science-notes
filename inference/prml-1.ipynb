{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRML L-1 \n",
    "# Introduction\n",
    "*Summary by Vaibhav Gupta (vvg239@nyu.edu)*\n",
    "\n",
    "The chapter starts by discussing how to find the best fit polynomial curve for given data points (observations) by adjusting the coefficients (weights) of the polynomial terms. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called linear models (the linearity is not with respect to observations).  \n",
    "\n",
    "The first error function considered here is the sum of squared errors between the predictions and target values. We see that when we take higher order polynomials, the error function decreases. As we keep going higher, we obtain an excellent fit to the training data. In fact, if polynomial is sufficiently high, it passes exactly through each data point and E(w) = 0. However, the fitted curve oscillates wildly and gives a very poor representation of the target function. This latter behaviour is known as **over-fitting**.  \n",
    "\n",
    "This feels paradoxical because lower order polynomials are special cases of higher order polynomials. But the reason for over-fitting is that when we increase the order of polynomial, the model has higher degrees of freedom and the weights get very finely tuned to the data by developing large positive and negative values so that the corresponding polynomial function matches each of the data points exactly, but between data points the function exhibits the large oscillations. What has happened is that the polynomial has fit the bias noise and actual stochastic noise with its huge degrees of freedom, instead of fitting the target function! The section also introduces the technique of weight decay for preventing the weights from assuming very large +ve/-ve values.\n",
    "\n",
    "\n",
    "## Probability Theory  \n",
    "\n",
    "Here we are first introduced to two very basic rules of probability : \n",
    "1. **Sum Rule** : The marginal probability $P(x)$ is the sum of all joints probabilities $P(x,y), \\forall y, X =x$  \n",
    "2. **Product Rule** : The join probability of $X$ and $Y$ is equal to conditional probability of $X$ w.r.t $Y$ multiplied by probability of $Y$.  \n",
    "\\begin{align}\n",
    "& P(X=x_i) = \\sum_{j=1}^{L}P(X=x_i,Y=y_j)\\\\\n",
    "& P(X,Y) = P(X | Y) P(Y)\\\\\n",
    "\\end{align}\n",
    "\n",
    "It also goes over other basics like definition of Exceptation and Covariance. Then it goes over the Bayes rule which states that the posterior distribution is directly proportionate to prior into likelihood.  \n",
    "\n",
    "Gaussian Probability Distribution : \n",
    "$$ P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "where $\\mu$ is the mean of the distribution and $\\sigma^2$ is the variance. Then we study how to find the mean and variance of a normal distribution given noisy observations from a Gaussian distribution.  \n",
    "\n",
    "\n",
    "## Decision Theory  \n",
    "\n",
    "Probability Theory tells us how to calculate the posterior distribution of a given input $x$ belonging to a particular class $C_k$ i.e. $P(C_k|x)$. Now how do we decide which class to categorize $x$ in given this posterior distribution? Decision Theory to the rescue.  \n",
    "\n",
    "If we want to minimize the misclassification rate, then we just assign $x$ to the class $C_k$ for which $P(C_k | x)$ is the maximum. But we don't always just want to minimize the misclassification rate. Sometimes, it is clearly better to make fewer mistakes of the second kind, even if this is at the expense of making more mistakes of the first kind. We can formalize such issues through the introduction of a loss function, also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred.\n",
    "\n",
    "In fact, we can identify three distinct approaches to solving decision problems, all of which have been used in practical applications.\n",
    "1. In the first approach we find the joint distribution of $x$ and $C_k$ i.e. $P(C_k|x)$. We then normalize to find the posterior distribution and base our classification decision on this posterior probability. Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.\n",
    "2. Here we find the posterior class probabilities and then use decision theory to find the class of $x$. Approaches that model the posterior probabilities directly are called discriminative models.\n",
    "3. Don't even deduce the posterior probability. Just find a function which classifies the observations into different classes. Like a perceptron. In this case, we won't know how confident we are of a classification. We will just know the final output.  \n",
    "\n",
    "Approach 1 is the most demanding as $X$ has a very high dimensionality and finding its joint distribution is very difficult. But if we have a generative model (Approach 1) we can generate data points (Duh!) and that is a big deal.  \n",
    "With approach 3 we directly get the needed class but we dont get a posterior distribution. Thus we miss out on many advantages including minimizing risk, reject option etc.  \n",
    "\n",
    "\n",
    "## Information Theory  \n",
    "\n",
    "Information Theory delves into how much information the value of random variable may provide to us. How do we quantify the amount of information received? It is equal to the factor by which the uncertainty (num of probable events) is reduced. More specifically, we see how many times the number of probable events is halved. This is known as the information gain (bits received). This is because each bit of information can halve the number of equally probable events.  \n",
    "\n",
    "For example, if we have eight equally likely events and a random variable tells us that only four of the eight events could have occured, then the information gain is equal to **one** (the number of probable events is halved once). If the random variable tells us exactly what event occured then the information gain is equal to **three** (the number of probable events is halved thrice, from 8 to 1).  \n",
    "\n",
    "What if all the events are not equally likely? Suppose the probability of day being sunny is 75% and being rainy is 25%. Now suppose a random variable tells us that the day is rainy. What is the information gain? One (as the num of probable events has been reduced from 2 to 1)? No, because we need to consider by how many times is the num of **equally probable** events halved!  \n",
    "\n",
    "We can see this as a day having 4 equally probable outcomes. Three outcomes are sunny and one outcome is rainy. Thus the variable reduces the number of equally probable outcomes from 4 to 1 and the information gained is **two** bits. How do we measure this? Suppose that a RV tells us that a event of probability $p$ has occured. The number of equally probable events of probability $p$ is $\\frac{1}{p}$. The number of equally probable events has been reduced from $\\frac{1}{p}$ to $1$ (it has been reduced by a factor of $p$).  By how many times has it been halved? $log_\\frac{1}{2}p$ or $-log_2p$. Thus, when a RV $X = x$, then the information gain is given by  \n",
    "$$h(x) = -log_2p(x)$$  \n",
    "\n",
    "What is the average amount of information gained from a random variable? It is given by the entropy of that random variable. $H$ is the entropy function and assigns higher value to a variable that is more random than another variable which assumes a particular value with very high probability.\n",
    "$$H[X] = E[h] = - \\sum_x{p(x)log_2p(x)}$$  \n",
    "\n",
    "More the randomness in a random variable, more its entropy. For cross-entropy and KL Divergence refer to [this video](https://youtu.be/ErfnhcEV1O8) from Hands-On Machine Learning.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
